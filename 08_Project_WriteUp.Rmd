---
title: "08_Identifying Proper Exercise Technique Using Accelerometer Data"
author: "Barb Dornseif - Saoirsegirl"
date: "April 23, 2015"
output:
  html_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 6
    keep_md: yes
    theme: readable
---
# Summary:  
The following analysis will evaluate a data set taken from a set of accelerometers placed on the belt, forearm, arm, and dumbell of six (6) participants. These six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:  
    - Class A: exactly according to the specification  
    - Class B: throwing the elbows to the front  
    - Class C: lifting the dumbbell only halfway  
    - Class D: lowering the dumbbell only halfway  
    - Class E: throwing the hips to the front  
Class A is considered a correct execution of the excercise.  

From this data set, a series of model algorithms were run and it was detirmined that a Random Forrest model yielded the best accuracy in predicting the outcomes on the test set. Due to the length of processing time for the models, the outcomes are either hard coded into the paper or pulled from a file of the saved model output. 

# Data Extraction, Formatting and Examinations
The data provided by the original study designers was made available at the [Weight Lifting Excercise summary page](http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv)  however for the class were were expected to use a shortened version for [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv ) and 20 random rows for [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).  

For purposes of this analysis the data was brought in as follows:
```{r set_globals, cache = TRUE, message=FALSE}
library(caret)
setwd("~/Documents/Coursera/08-PracticalMachineLearning/08_Project")
```  
```{r import, eval=FALSE, echo=TRUE}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, "./Data/training.csv", method = "curl")
download.file(testURL, "./Data/testing.csv", method = "curl")
```  

The training data file was loaded into R with the name *trainRaw*. Several iterations were run to successfully load the data in a manner that allowed for subsequent tidying, and model building. To aid these iterations, a data.frame was created with the names, formats, and existance of NAs - *trainRaw_str*. This data.frame was used to apply logic for "Usability", which in turn provided the list of usable columns.  

The functions summary(), str(), and head() were used as well. For brevity, the iterations of functions is use are not included here. The goal of these steps was to remove any column with a high percentage of NAs or blank inputs in order to shorten the processing time of subsequent models. This step reduced the raw file width from 160 columns to 53 columns of data. The new file *use* was then used to complete the segmentation for training and validation. Later, the test set of 20 hold out rows was processed in the same manner.  

``` {r LoadAndTrim, cache=TRUE}
## Load the data - iterations were run to detirmine final function variables
trainRaw <- read.table("./Data/training.csv", header = TRUE, sep = ",",
                    stringsAsFactors = FALSE) # we will transform the outcome to a factor variable later
# first make a data.frame of the name, class, and existance of NA for each column
cNames <- names(trainRaw)
cClass <- as.character()
for (i in 1:dim(trainRaw)[2])  {cClass <- c(cClass, class(trainRaw[ , i]))}
hasNA <- as.character()
for (i in 1:dim(trainRaw)[2])  {
    hasNA <- c(hasNA, anyNA(trainRaw[ , i]))}
trainRaw_str <- as.data.frame(cbind(cNames = cNames, cClass = cClass, hasNA = hasNA), stringsAsFactors = FALSE)

# now evaluate the metrics for suitability for use - in this case having no NA and being a metric
useCol <- as.numeric()
for (i in 8:159) {
    if (trainRaw_str[i, 2] != "character") {
        if (trainRaw_str[i, 3] == "FALSE") {
            useCol <- c(useCol, as.numeric(i))  } } }

# create the usable training set and a pre-test evaluator set
use <- trainRaw[ , c(160, useCol)] # Place Output in the first column
use[ ,1] <- as.factor(use[ , 1]) # convert to Factor variable for better porcessing
set.seed <- 432
inTrain <- createDataPartition(y=use$classe, # vector of outcomes
                               p = 0.80, # Percentage to training
                               list=FALSE) 
training <- use[inTrain, ] 
compare <- use[ - inTrain, ]  # we are not "testing" here so I am renaming this hold out set.
summary(training)
```
We can verify here that the 53 columns are well formed and the data is reasonable to use in a model training function.  

# Model Selection and Evaluation

Given that the output prediction for this data set is a classification of five possible choices, I focused on comparing models that handle classifications - Trees, Random Forrest, and GBM. I had hoped that a basic Tree model would have an acceptable in-sample error rate as it is the easiest model to help explain the influence of the choosen predictors.  Unfortunately the in-sample error rate was quite high, just over 50%. It therefore did not warrant more time and effort. Here is the code that was run. 
```{r tree, eval=FALSE}
fitBit_tree <- train(classe ~ . , method = "rpart", data = training)
print(fitBit_tree$finalModel)
# Visualize tree
par(mfrow = c(1,1))
plot(fitBit_tree$finalModel, uniform = TRUE)
text(fitBit_tree$finalModel, use.n = TRUE, all=TRUE, cex = 1)
predict_tree <- predict(fitBit_tree,newdata=compare)
confusionMatrix(predict_tree, compare$classe)
resample_tree <- print(fitBit_tree$resample)
hist(resample_tree[[ , 1])
# Accuracy .4922
```  
The second model I tested was the Random Forrest using the defaults in the train() function. Most notably it uses the Bootstrap method of resampling for variable selection and cross-validation.  
``` {r Random Forrest, eval = FALSE}
fitBit_rf <- train(classe ~ ., data = training, method = "rf", prox=TRUE)
```
I have saved the output of that model - *fitBit_rf.rda* - and will use it to document the performance of the model here. We see that peak accuracy is acheived at 27 of 52 predictors being used and that error rate redcution is achieved using ~50 trees (sorry plot won't render in Rmd). To reduce processing time of future models, I would recommend those parameters being added to the train.control options of train().  
``` {r load-rf, cache=TRUE}
setwd("~/Documents/Coursera/08-PracticalMachineLearning/08_Project/Answers")
load("fitBit_rf.rda")
# plot(fitBit_rf$finalModel) # will not plot using loaded model file via Rmd.
plot(fitBit_rf,  main = "Accuracy by No. of Predictors", lwd = 2)
resample_rf <- print(fitBit_rf$resample)
```  
The list of 25 resamplings shows us how stable the accuracy is across the 25 bootstraps of the training data.  This bodes well for a reduced out-of-sample error when testing against the validation and final test sets of data.

The resulting model was fed through the predict() function using the validation set of data to evaluate the out-of-sample error rate prior to final testing.
``` {r predict_rf, messages=FALSE}
library(caret)
predict_rf <- predict(fitBit_rf, newdata=compare)
confusionMatrix(predict_rf, compare$classe)
```
The confusion Matrix shows our error rate for this unrelated data set was 99.85%. Given our use of bootstrapping in the model (and the performance hit that involves), and a validation accuracy that was in line with the model accuracy, further cross-validation iterations using a separate set.seed() with each iteration of data splitting was not pursued.  

As this is so high, it seemed silly to run the GBM model, but I did.

``` {r gbm, eval=FALSE}
fitBit_gbm <- train(classe ~ ., method = 'gbm', data = training, verbose = FALSE)
resample_gbm <- print(fitBit_gbm$resample)
predict_gbm <- predict(fitBit_gbm, newdata=compare)
confusionMatrix(predict_gbm, compare$classe)
# Accuracy = 0.9643
```  
While still a respectable 96.43% accurate against the validation set, the GBM did not perform as well as the Random Forrest model.

# Expected Out of Sample Error Rate For the 20 Row Test Data Set
Given the 99+% accuracy of the Random Forrest model against a large set of validation data, and given that points were on the line, I hypothesized a 100% accuracy in predicting the 20 test cases.  If only one was incorrect, that would result in a 95% accuracy rate which was outside the condfidence interval.  Plus it would be mean to "steal" away a point :-) 

# Predictions Against the Test Set  
The choosen model was used to generate the needed files for submission.  
```{r submit, eval=FALSE}
# Generate vector of predictions for submission into grading engine
# Apply all data procedures use on training set to the test set.
testRaw <- read.table("./Data/testing.csv", header = TRUE, sep = ",",
                       stringsAsFactors = FALSE)
use_test <- testRaw[ , c(160, useCol)]
use_test[ ,1] <- as.factor(use_test[ , 1])
str(use_test[ ,1])
predict_submit <- predict(fitBit_rf, newdata=use_test)
answers <- as.character(predict_submit)

# create files for submission
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(answers)
```  

## Thanks and Acknowledgement of the Original Experiment Designers
Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3YCIDMg00
Citations and Thanks - Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.  "Qualitative Activity Recognition of Weight Lifting Exercises" Read more: http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201#ixzz3YCGaaFml 